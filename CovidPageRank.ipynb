{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andiub97/CovidPubRank/blob/master/CovidPageRank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAM6vyXAfVUj"
      },
      "source": [
        "# Page Ranking Algorithms on Google Cloud Dataproc\n",
        "\n",
        "- Use the [Cloud Resource Manager](https://cloud.google.com/resource-manager) to create a project if you do not already have one.\n",
        "- Enable Dataproc and Cloud Storage services for the project \n",
        "- [Enable billing](https://support.google.com/cloud/answer/6293499#enable-billing) for the project.\n",
        "- See [Google Cloud Storage (GCS) Documentation](https://cloud.google.com/storage/) for more info.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Environment variables(optional) \n",
        "Here you can choose if install colab-env dependency and allow Google Drive to access to your Drive and create environment file containing enviroment variables that you can choose for all later tasks involving bucket, cluster properties and so on."
      ],
      "metadata": {
        "id": "rl2goHQ9bGwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --upgrade"
      ],
      "metadata": {
        "id": "A5P4toB-jEy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "\n",
        "colab_env.envvar_handler.add_env(\"GOOGLE_PROJECT_ID\", \"insert_project-name\", overwrite=True)\n",
        "colab_env.envvar_handler.add_env(\"DATASET_BUCKET\", \"insert_input-file-bucket-name\", overwrite=True)\n",
        "colab_env.envvar_handler.add_env(\"OUTPUT_BUCKET\", \"insert_output-file-bucket-name\", overwrite=True)\n",
        "colab_env.envvar_handler.add_env(\"CLUSTER_REGION\", \"insert_cluster-regione-name\", overwrite=True)\n",
        "colab_env.envvar_handler.add_env(\"CLUSTER_ZONE\", \"insert_cluster-zone-name\", overwrite=True)\n",
        "\n",
        "!more gdrive/My\\ Drive/vars.env"
      ],
      "metadata": {
        "id": "OIVXxR2HjBG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Allow Google Cloud access to the notebook and set GC project for this session"
      ],
      "metadata": {
        "id": "G3Q0bQ5Xc60Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LADpx7LReOMk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = os.getenv(\"GOOGLE_PROJECT_ID\")\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone CovidPubRank repo from Github, unzip citation archive and move files to \"citations\" folder "
      ],
      "metadata": {
        "id": "h9smrQv5Bj6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "!git clone https://github.com/andiub97/CovidPubRank.git\n",
        "\n",
        "tar = tarfile.open(\"./CovidPubRank/data/data.tar.gz\")\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "!mkdir ./sample_data/datasets\n",
        "!mv ./dataset_1015681.txt ./sample_data/datasets\n",
        "!mv ./dataset_32685.txt ./sample_data/datasets\n",
        "!mv ./dataset_14924.txt ./sample_data/datasets\n",
        "!mv ./dataset_9647.txt ./sample_data/datasets"
      ],
      "metadata": {
        "id": "-TpHyo0EBkOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create buckets and load datasets and jar file into them\n",
        "Create a bucket and upload datasets into it.\n",
        "Remember create a bucket for jar file and upload it using GCP Graphic Interface or shell, but we suggest using Google Cloud CLI by your machine"
      ],
      "metadata": {
        "id": "oWnhIirNq0ft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8LMNz9SR0Iz"
      },
      "outputs": [],
      "source": [
        "bucket_name = os.getenv(\"DATASET_BUCKET\")\n",
        "\n",
        "!gsutil mb -l us-central1 -b on gs://{bucket_name}\n",
        "\n",
        "# Copy files to new bucket.\n",
        "!gsutil cp ./sample_data/datasets/dataset_9647.txt gs://{bucket_name}/\n",
        "!gsutil cp ./sample_data/datasets/dataset_14924.txt gs://{bucket_name}/\n",
        "!gsutil cp ./sample_data/datasets/dataset_32685.txt gs://{bucket_name}/\n",
        "!gsutil cp ./sample_data/datasets/dataset_1015681.txt gs://{bucket_name}/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create output bucket for storing algorithms statistics"
      ],
      "metadata": {
        "id": "joeHd_6xlFYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_bucket_name = os.getenv(\"OUTPUT_BUCKET\")\n",
        "!gsutil mb -l us-central1 -b on gs://{output_bucket_name}"
      ],
      "metadata": {
        "id": "-frvJ8Yck73c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weak Scalability \n",
        "### Perform all algorithms on same cluster varying in dataset size"
      ],
      "metadata": {
        "id": "dGeuyUSetuHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create single node cluster"
      ],
      "metadata": {
        "id": "PBtG5xoRn1fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region = os.getenv(\"CLUSTER_REGION\")\n",
        "zone = os.getenv(\"CLUSTER_ZONE\")\n",
        "\n",
        "!gcloud dataproc clusters create single-node-cluster \\\n",
        "  --region {region} \\\n",
        "  --zone {zone} \\\n",
        "  --single-node "
      ],
      "metadata": {
        "id": "K1k9UtnlORQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Send Dataproc jobs to single-node cluster for all datasets and algorithms"
      ],
      "metadata": {
        "id": "ZtOdnUccBa2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc jobs submit spark \\\n",
        "    --cluster=single-node-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")} \\\n",
        "    --jar=gs://covid_program/covidpubrank_2.12-0.1.0-SNAPSHOT.jar \\\n",
        "    -- \"local\" \"AllAlgorithms\" {os.getenv(\"DATASET_BUCKET\")}\"dataset_9647.txt\" {os.getenv(\"OUTPUT_BUCKET\")}\"single-node/dataset_9647\" \"4\" \"localOnCloud\" \"0\""
      ],
      "metadata": {
        "id": "dVfeGTQgMuyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc jobs submit spark \\\n",
        "    --cluster=single-node-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")} \\\n",
        "    --jar=gs://covid_program/covidpubrank_2.12-0.1.0-SNAPSHOT.jar \\\n",
        "    -- \"local\" \"AllAlgorithms\" {os.getenv(\"DATASET_BUCKET\")}\"dataset_14924.txt\" {os.getenv(\"OUTPUT_BUCKET\")}\"single-node/dataset_14924\" \"4\" \"localOnCloud\" \"0\""
      ],
      "metadata": {
        "id": "LX4E6XClzOyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc jobs submit spark \\\n",
        "    --cluster=single-node-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")} \\\n",
        "    --jar=gs://covid_program/covidpubrank_2.12-0.1.0-SNAPSHOT.jar \\\n",
        "    -- \"local\" \"AllAlgorithms\" {os.getenv(\"DATASET_BUCKET\")}\"dataset_32685.txt\" {os.getenv(\"OUTPUT_BUCKET\")}\"single-node/dataset_32685\" \"4\" \"localOnCloud\" \"0\""
      ],
      "metadata": {
        "id": "-CZI9CYPzWkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc jobs submit spark \\\n",
        "    --cluster=single-node-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")} \\\n",
        "    --jar=gs://covid_program/covidpubrank_2.12-0.1.0-SNAPSHOT.jar \\\n",
        "    -- \"local\" \"AllAlgorithms\" {os.getenv(\"DATASET_BUCKET\")}\"dataset_1015681.txt\" {os.getenv(\"OUTPUT_BUCKET\")}\"single-node/dataset_1015681\" \"4\" \"localOnCloud\" \"0\""
      ],
      "metadata": {
        "id": "BYhQ5oyN9ZU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete cluster"
      ],
      "metadata": {
        "id": "-_SA-f6Jt0hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters delete single-node-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")}"
      ],
      "metadata": {
        "id": "jx5e1dZTOf7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Strong scalability\n",
        "### Perform all algorithms on same dataset varying number of workers per cluster"
      ],
      "metadata": {
        "id": "ZAR03op7MCaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create 2-workers cluster with n1-standard-4 machines"
      ],
      "metadata": {
        "id": "l6af6shLoWYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters create two-workers-cluster \\\n",
        "  --region {os.getenv(\"CLUSTER_REGION\")} \\\n",
        "  --zone {os.getenv(\"CLUSTER_ZONE\")} \\\n",
        "  --master-machine-type n1-standard-4\\\n",
        "  --master-boot-disk-size 500 \\\n",
        "  --worker-machine-type n1-standard-4 \\\n",
        "  --num-workers 2 \\\n",
        "  --worker-boot-disk-size 500"
      ],
      "metadata": {
        "id": "Ck6Aa1xQJONv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Send Dataproc jobs performing DistributedPageRank and ParallelPageRankLibrary algorithms on \"dataset_1015681.txt\" dataset"
      ],
      "metadata": {
        "id": "CDox9xeOBq1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc jobs submit spark \\\n",
        "    --cluster=two-workers-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")} \\\n",
        "    --jar=gs://covid_program/covidpubrank_2.12-0.1.0-SNAPSHOT.jar \\\n",
        "    -- \"yarn\" \"DistributedAlgorithms\" {os.getenv(\"DATASET_BUCKET\")}\"dataset_1015681.txt\" {os.getenv(\"OUTPUT_BUCKET\")}\"two-nodes-4/distributed\" \"16\" \"distributedOnCloud\" \"two_workers_n1_standard_4\""
      ],
      "metadata": {
        "id": "inxuN4VGJbkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete cluster"
      ],
      "metadata": {
        "id": "ERERN4oqjgwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters delete two-workers-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")}"
      ],
      "metadata": {
        "id": "2OfvzXodjn7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create 2-workers cluster with n1-standard-8 machines"
      ],
      "metadata": {
        "id": "Y_7bTBp3jSoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters create two-workers-cluster \\\n",
        "  --region {os.getenv(\"CLUSTER_REGION\")} \\\n",
        "  --zone {os.getenv(\"CLUSTER_ZONE\")} \\\n",
        "  --master-machine-type n1-standard-8\\\n",
        "  --master-boot-disk-size 500 \\\n",
        "  --worker-machine-type n1-standard-8 \\\n",
        "  --num-workers 2 \\\n",
        "  --worker-boot-disk-size 500"
      ],
      "metadata": {
        "id": "1co1S-AQNczk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Send Dataproc jobs performing DistributedPageRank and ParallelPageRankLibrary algorithms on \"dataset_1015681.txt\" dataset"
      ],
      "metadata": {
        "id": "LvLLswKDjU7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc jobs submit spark \\\n",
        "    --cluster=two-workers-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")} \\\n",
        "    --jar=gs://covid_program/covidpubrank_2.12-0.1.0-SNAPSHOT.jar \\\n",
        "    -- \"yarn\" \"DistributedAlgorithms\" {os.getenv(\"DATASET_BUCKET\")}\"dataset_1015681.txt\" {os.getenv(\"OUTPUT_BUCKET\")}\"two-nodes-8/distributed\" \"16\" \"distributedOnCloud\" \"two_workers_n1_standard_8\""
      ],
      "metadata": {
        "id": "DHZ2iol7NMsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete cluster"
      ],
      "metadata": {
        "id": "mZF8ijj_Or6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters delete two-workers-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")}"
      ],
      "metadata": {
        "id": "fMKvkvnlOrUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create 4-workers cluster"
      ],
      "metadata": {
        "id": "5nTHKPDLogD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters create four-workers-cluster \\\n",
        "  --region {os.getenv(\"CLUSTER_REGION\")} \\\n",
        "  --zone {os.getenv(\"CLUSTER_ZONE\")} \\\n",
        "  --master-machine-type n1-standard-4 \\\n",
        "  --master-boot-disk-size 500 \\\n",
        "  --worker-machine-type n1-standard-4 \\\n",
        "  --num-workers 4\\\n",
        "  --worker-boot-disk-size 500 \\\n"
      ],
      "metadata": {
        "id": "e1gfySgoPAFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Send Dataproc jobs performing DistributedPageRank and ParallelPageRankLibrary algorithms on \"dataset_1015681.txt\" dataset"
      ],
      "metadata": {
        "id": "X2gxa84ikWrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc jobs submit spark \\\n",
        "    --cluster=four-workers-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")} \\\n",
        "    --jar=gs://covid_program/covidpubrank_2.12-0.1.0-SNAPSHOT.jar \\\n",
        "    -- \"yarn\" \"DistributedAlgorithms\" {os.getenv(\"DATASET_BUCKET\")}\"dataset_1015681.txt\" {os.getenv(\"OUTPUT_BUCKET\")}\"four-nodes/distributed\" \"16\" \"distributedOnCloud\" \"four_workers_n1_standard_4\""
      ],
      "metadata": {
        "id": "xqSzsgr2Js4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete cluster"
      ],
      "metadata": {
        "id": "Qe_JnYRSlA3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters delete four-workers-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")}"
      ],
      "metadata": {
        "id": "YHZupG9plEY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create 5-workers cluster"
      ],
      "metadata": {
        "id": "ChBr0Y5Mkcfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters create five-workers-cluster \\\n",
        "  --region {os.getenv(\"CLUSTER_REGION\")} \\\n",
        "  --zone {os.getenv(\"CLUSTER_ZONE\")} \\\n",
        "  --master-machine-type n1-standard-4 \\\n",
        "  --master-boot-disk-size 500 \\\n",
        "  --worker-machine-type n1-standard-4 \\\n",
        "  --num-workers 5 \\\n",
        "  --worker-boot-disk-size 500 \\"
      ],
      "metadata": {
        "id": "pdEPMML2JqSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Send Dataproc jobs performing DistributedPageRank and ParallelPageRankLibrary algorithms on \"dataset_1015681.txt\" dataset"
      ],
      "metadata": {
        "id": "UNanDhvENJsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc jobs submit spark \\\n",
        "    --cluster=five-workers-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")} \\\n",
        "    --jar=gs://covid_program/covidpubrank_2.12-0.1.0-SNAPSHOT.jar \\\n",
        "    -- \"yarn\" \"DistributedAlgorithms\" {os.getenv(\"DATASET_BUCKET\")}\"dataset_1015681.txt\" {os.getenv(\"OUTPUT_BUCKET\")}\"five-nodes/distributed\" \"16\" \"distributedOnCloud\" \"five_workers_n1_standard_4\""
      ],
      "metadata": {
        "id": "LvQqWqh2O0u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete cluster"
      ],
      "metadata": {
        "id": "cPOebsqnQ7jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters delete five-workers-cluster \\\n",
        "    --region={os.getenv(\"CLUSTER_REGION\")}"
      ],
      "metadata": {
        "id": "TENg3A1hMeRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get jobs list and delete job specifying its id"
      ],
      "metadata": {
        "id": "yUo2qNw7hy4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set dataproc/region {os.getenv(\"CLUSTER_REGION\")}\n",
        "listJob = !gcloud dataproc jobs list --format='value(JOB_ID)'\n",
        "\n",
        "for i in listJob:\n",
        "  !gcloud dataproc jobs delete {i}"
      ],
      "metadata": {
        "id": "5ORsXLLxmYj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get job execution output, in other words get ranking algorithms' execution time and plot them"
      ],
      "metadata": {
        "id": "rwlbBc0zt30Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r \"gs://output_bucket_results\" ."
      ],
      "metadata": {
        "id": "xS8sJFWDGQu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./output_bucket_results/five-nodes/distributed/part-00000\") as textfile1, open(\"./output_bucket_results/four-nodes/distributed/part-00000\") as textfile2, open(\"./output_bucket_results/two-nodes/distributed/part-00000\") as textfile3, open(\"./output_bucket_results/two-nodes-8/distributed/part-00000\") as textfile4: \n",
        "    data1 = textfile1.read()\n",
        "    data2 = textfile2.read()\n",
        "    data3 = textfile3.read()\n",
        "    data4 = textfile4.read()\n",
        "\n",
        "\n",
        "data1 += data2\n",
        "data1 += data3\n",
        "data1 += data4\n",
        "\n",
        "\n",
        "with open(\"./output_bucket_results/strong-scalability-result.txt\", 'w') as textfile5:\n",
        "\n",
        "    textfile5.write(data1)"
      ],
      "metadata": {
        "id": "rl9ITNZZeus7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./output_bucket_results/single-node/dataset_1015681/part-00000\") as textfile1, open(\"./output_bucket_results/single-node/dataset_14924/part-00000\") as textfile2, open(\"./output_bucket_results/single-node/dataset_32685/part-00000\") as textfile3, open(\"./output_bucket_results/single-node/dataset_9647/part-00000\") as textfile4: \n",
        "    data1 = textfile1.read()\n",
        "    data2 = textfile2.read()\n",
        "    data3 = textfile3.read()\n",
        "    data4 = textfile4.read()\n",
        "\n",
        "data1 += \"\\n\"+ data2\n",
        "data1 += data3 \n",
        "data1 += data4\n",
        "print (data1)\n",
        "with open(\"./output_bucket_results/weak-scalability-result.txt\", 'w') as textfile5:\n",
        "\n",
        "    textfile5.write(data1)"
      ],
      "metadata": {
        "id": "cTAjNwEIQpa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot execution time of algorithms showing weak scalability"
      ],
      "metadata": {
        "id": "rJ4FRSC-snJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt                                                \n",
        "\n",
        "names = []\n",
        "times = []\n",
        "dataset_name = []\n",
        "chars = \"()\\n\"\n",
        "\n",
        "results1 = {'dataset_9647.txt': {\n",
        "       'ranking.DistributedPageRank': '0.0',\n",
        "       'ranking.ParallelPageRankLibrary': '0.0',\n",
        "       'ranking.PageRank':'0.0',\n",
        "       'ranking.PageRankLibrary':'0.0'\n",
        "   },\n",
        "   'dataset_14924.txt': {\n",
        "       'ranking.DistributedPageRank': '0.0',\n",
        "       'ranking.ParallelPageRankLibrary': '0.0',\n",
        "       'ranking.PageRank':'0.0',\n",
        "       'ranking.PageRankLibrary':'0.0'\n",
        "   },\n",
        "   'dataset_32685.txt': {\n",
        "       'ranking.DistributedPageRank': '0.0',\n",
        "       'ranking.ParallelPageRankLibrary': '0.0',\n",
        "       'ranking.PageRank':'0.0',\n",
        "       'ranking.PageRankLibrary':'0.0'\n",
        "   },\n",
        "   'dataset_1015681.txt': {\n",
        "       'ranking.DistributedPageRank': '0.0',\n",
        "       'ranking.ParallelPageRankLibrary': '0.0',\n",
        "       'ranking.PageRank':'0.0',\n",
        "       'ranking.PageRankLibrary':'0.0'\n",
        "   }\n",
        "}\n",
        "\n",
        "f = open(\"./output_bucket_results/weak-scalability-result.txt\",'r')\n",
        "     \n",
        "    \n",
        "\n",
        "for row in f:\n",
        "  for c in chars:\n",
        "    row = row.replace(c,\"\")\n",
        "  for c in \",\":\n",
        "    row = row.replace(c,\" \")\n",
        "    row = row.split(' ')\n",
        "  if (\"dataset_9647.txt\" == row[2]):\n",
        "    results1[\"dataset_9647.txt\"][row[0]] = (row[1])\n",
        "  if (\"dataset_14924.txt\" == row[2]):\n",
        "    results1[\"dataset_14924.txt\"][row[0]] = (row[1])\n",
        "  if (\"dataset_32685.txt\" == row[2]):\n",
        "    results1[\"dataset_32685.txt\"][row[0]] = (row[1])    \n",
        "  if (\"dataset_1015681.txt\" == row[2]):\n",
        "    results1[\"dataset_1015681.txt\"][row[0]] = (row[1])\n",
        "\n",
        "# set width of bar\n",
        "barWidth = 0.1\n",
        "fig = plt.subplots(figsize =(16, 9))\n",
        "\n",
        "# set height of bar\n",
        "dataset_9647 = []\n",
        "for i in results1[\"dataset_9647.txt\"]:\n",
        "    dataset_9647.append(float(results1[\"dataset_9647.txt\"][i]))\n",
        "\n",
        "dataset_14924 = []\n",
        "for i in results1[\"dataset_14924.txt\"]:\n",
        "    dataset_14924.append(float(results1[\"dataset_14924.txt\"][i]))\n",
        "\n",
        "dataset_32685 = []\n",
        "for i in results1[\"dataset_32685.txt\"]:\n",
        "    dataset_32685.append(float(results1[\"dataset_32685.txt\"][i]))\n",
        "\n",
        "dataset_1015681 = []\n",
        "for i in results1[\"dataset_1015681.txt\"]:\n",
        "    dataset_1015681.append(float(results1[\"dataset_1015681.txt\"][i]))\n",
        "\n",
        "# Set position of bar on X axis\n",
        "br1 = np.arange(len(dataset_9647))\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "br4 = [x + barWidth for x in br3]\n",
        "\n",
        "# Make the plot\n",
        "plt.bar(br1, dataset_9647, color ='r', width = barWidth,\n",
        "\t\tedgecolor ='black', label ='dataset_9647.txt')\n",
        "\n",
        "plt.bar(br2, dataset_14924, color ='g', width = barWidth,\n",
        "\t\tedgecolor ='black', label ='dataset_14924.txt')\n",
        "\n",
        "plt.bar(br3, dataset_32685, color ='b', width = barWidth,\n",
        "\t\tedgecolor ='black', label ='dataset_32685.txt')\n",
        "\n",
        "plt.bar(br4, dataset_1015681, color ='yellow', width = barWidth,\n",
        "\t\tedgecolor ='black', label ='dataset_1015681.txt')\n",
        "\n",
        "# Adding Xticks\n",
        "plt.xlabel('Algorithms Names', fontweight ='bold', fontsize = 10)\n",
        "plt.ylabel('Execution time [s] in log scale', fontweight ='bold', fontsize = 10)\n",
        "# Setting a logarithmic scale for y-axis\n",
        "x = results1[\"dataset_9647.txt\"]\n",
        "\n",
        "ticks = []\n",
        "for k in x.keys():\n",
        "  if k:\n",
        "      ticks.append(k)\n",
        "plt.xticks([r + barWidth for r in range(len(br1))],ticks)\n",
        "plt.yscale('log')\n",
        "plt.ylim(1)\n",
        "plt.legend()\n",
        "plt.title(\"Weak scalability performance\", fontdict = {'fontsize' : 20})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZSzcqEunvW24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot execution time of algorithms showing strong scalability"
      ],
      "metadata": {
        "id": "kfSwKYGPlf89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt                                                \n",
        "\n",
        "names = []\n",
        "times = []\n",
        "dataset_name = []\n",
        "chars = \"()\\n\"\n",
        "\n",
        "stats = {\n",
        "   \"two_workers_n1_standard_4\":{\n",
        "      \"ranking.DistributedPageRank\":\"0.0\",\n",
        "      \"ranking.ParallelPageRankLibrary\":\"0.0\"\n",
        "   },\n",
        "   \"two_workers_n1_standard_8\":{\n",
        "      \"ranking.DistributedPageRank\":\"0.0\",\n",
        "      \"ranking.ParallelPageRankLibrary\":\"0.0\"\n",
        "   },\n",
        "   \"four_workers_n1_standard_4\":{\n",
        "      \"ranking.DistributedPageRank\":\"0.0\",\n",
        "      \"ranking.ParallelPageRankLibrary\":\"0.0\"\n",
        "   },\n",
        "    \"five_workers_n1_standard_4\":{\n",
        "      \"ranking.DistributedPageRank\":\"0.0\",\n",
        "      \"ranking.ParallelPageRankLibrary\":\"0.0\"\n",
        "   }\n",
        "}\n",
        "\n",
        "f = open('./output_bucket_results/strong-scalability-result.txt','r')\n",
        "     \n",
        "    \n",
        "for row in f:\n",
        "  for c in chars:\n",
        "    row = row.replace(c,\"\")\n",
        "  for c in \",\":\n",
        "    row = row.replace(c,\" \")\n",
        "    row = row.split(' ')\n",
        "  if (\"two_workers_n1_standard_4\" == row[3]):\n",
        "    stats[\"two_workers_n1_standard_4\"][row[0]] = (row[1])\n",
        "  if (\"two_workers_n1_standard_8\" == row[3]):\n",
        "    stats[\"two_workers_n1_standard_8\"][row[0]] = (row[1])\n",
        "  if (\"four_workers_n1_standard_4\" == row[3]):\n",
        "    stats[\"four_workers_n1_standard_4\"][row[0]] = (row[1])\n",
        "  if (\"five_workers_n1_standard_4\" == row[3]):\n",
        "    stats[\"five_workers_n1_standard_4\"][row[0]] = (row[1])\n",
        "\n",
        "# set width of bar\n",
        "barWidth = 0.05\n",
        "fig = plt.subplots(figsize = (16, 9))\n",
        "\n",
        "# set height of bar\n",
        "two_workers_n1_standard_4 = []\n",
        "for i in stats[\"two_workers_n1_standard_4\"]:   \n",
        "  two_workers_n1_standard_4.append(float(stats[\"two_workers_n1_standard_4\"][i]))\n",
        "\n",
        "two_workers_n1_standard_8 = []\n",
        "for i in stats[\"two_workers_n1_standard_8\"]:   \n",
        "  two_workers_n1_standard_8.append(float(stats[\"two_workers_n1_standard_8\"][i]))\n",
        "\n",
        "four_workers_n1_standard_4 = []\n",
        "for i in stats[\"four_workers_n1_standard_4\"]:   \n",
        "  four_workers_n1_standard_4.append(float(stats[\"four_workers_n1_standard_4\"][i]))\n",
        "\n",
        "five_workers_n1_standard_4 = []\n",
        "for i in stats[\"five_workers_n1_standard_4\"]:   \n",
        "  five_workers_n1_standard_4.append(float(stats[\"five_workers_n1_standard_4\"][i]))\n",
        "\n",
        "# Set position of bar on X axis\n",
        "br1 = np.arange(len(two_workers_n1_standard_4))\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "br4 = [x + barWidth for x in br3]\n",
        "\n",
        "# Make the plot\n",
        "plt.bar(br1, two_workers_n1_standard_4, color ='r', width = barWidth,\n",
        "\t\tedgecolor ='black', label ='2 workers n1-standard-4')\n",
        "\n",
        "plt.bar(br2, two_workers_n1_standard_8, color ='g', width = barWidth,\n",
        "\t\tedgecolor ='black', label ='2 workers n1-standard-8')\n",
        "\n",
        "plt.bar(br3, four_workers_n1_standard_4, color ='b', width = barWidth,\n",
        "\t\tedgecolor ='black', label ='4 workers n1-standard-4')\n",
        "\n",
        "plt.bar(br4, five_workers_n1_standard_4, color ='yellow', width = barWidth,\n",
        "\t\tedgecolor ='black', label ='5 workers n1-standard-4')\n",
        "\n",
        "# Adding Xticks\n",
        "plt.xlabel('Algorithms Names', fontweight ='bold', fontsize = 10)\n",
        "plt.ylabel('Execution time [s]', fontweight ='bold', fontsize = 10)\n",
        "# Setting a logarithmic scale for y-axis\n",
        "x = stats[\"two_workers_n1_standard_4\"]\n",
        "\n",
        "ticks = []\n",
        "for k in x.keys():\n",
        "  if k:\n",
        "      ticks.append(k)\n",
        "\n",
        "plt.xticks([r + barWidth for r in range(len(br1))],ticks)\n",
        "plt.title(\"Algorithms result with 75648912 edges dataset\", fontdict = {'fontsize' : 20})\n",
        "plt.legend()\n",
        "plt.show()\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DZyTU-Fof5wW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copia di CovidPageRank.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}